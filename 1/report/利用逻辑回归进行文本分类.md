# 任务一：基于逻辑回归的文本分类

利用简单的机器学习模型构建文本分类模型。要求仅使用numpy，pandas库，不能使用框架。我们利用词袋模型构建特征，创建训练集与测试集，同时创建回归模型，对模型进行训练与预测。

数据集：[Classify the sentiment of sentences from the Rotten Tomatoes dataset](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)

## 步骤

### 特征构建

我们基于单词的统计模型构建特征，简称BOW。而BOW又分为是按照单词建模还是按照字符建模。不管是那种，第一步根据要求把```feature_set```构建出来。

```python

def NGram_tokenize(data,ngram=2):
    # ngram特征集合
    feature_set=set()
    for row in data:
        token_list=row.split()
        # 对每个词提取ngram特征
        for token in token_list:
            for i in range(len(token)-ngram):
                feature_set.add(token[i:i+ngram])
    return feature_set
```

第二步对训练与测试集进行特征的提取。

```python
def NGram_creat_feature(data,feature_set,ngram=2):
    feature_size=len(feature_set)
    feature_list=np.zeros((data.shape[0],feature_size)).astype('int16')
    feature_map=dict(zip(feature_set,range(feature_size)))
    for index in range(data.shape[0]):
        token_list=data[index].split()
        for token in token_list:
            for i in range(len(token)-ngram):
                gram=token[i:i+ngram]
                if gram in feature_set:
                    feature_index=feature_map[gram]
                    feature_list[index,feature_index]+=1
    return feature_list
```

### 数据集划分

将训练集划分为训练集和验证集。

```python
def train_test_split(X,y,test_rate):
    X_size=X.shape[0]
    train_size=int((1-test_rate)*X_size)
    index = [i for i in range(X_size)] 
    random.shuffle(index)
    train_X = X[index][0:train_size]
    train_y = y[index][0:train_size]
    val_X = X[index][train_size+1:-1]
    val_y = y[index][train_size+1:-1]
    return train_X,val_X,train_y,val_y
```



### 模型训练

我们首先需要知道我们采用的模型的公式表达，损失函数表达，参数更新方式。

SoftMax Regression 公式（P75）：
$$
\begin{aligned}
\hat{\boldsymbol{y}} &=\operatorname{softmax}\left(\boldsymbol{W}^{\top} \boldsymbol{x}\right) \\
&=\frac{\exp \left(\boldsymbol{W}^{\top} \boldsymbol{x}\right)}{\mathbf{1}_{C}^{\top} \exp \left(\boldsymbol{W}^{\top} \boldsymbol{x}\right)}
\end{aligned}
$$
其中 $\boldsymbol{W}=\left[\boldsymbol{w}_{1}, \cdots, \boldsymbol{w}_{C}\right]$ 是由 $C$ 个类的权重向量组成的矩阵, $\mathbf{1}_{C}$ 为 $C$ 维的全 1 向 量, $\hat{\boldsymbol{y}} \in \mathbb{R}^{C}$ 为所有类别的预测条件概率组成的向量, 第 $c$ 维的值是第 $c$ 类的预测 条件概率.

损失函数（p76）：
$$
\begin{aligned}
\mathcal{R}(\boldsymbol{W}) &=-\frac{1}{N} \sum_{n=1}^{N} \sum_{c=1}^{C} \boldsymbol{y}_{c}^{(n)} \log \hat{\boldsymbol{y}}_{c}^{(n)} \\
&=-\frac{1}{N} \sum_{n=1}^{N}\left(\boldsymbol{y}^{(n)}\right)^{\mathrm{T}} \log \hat{\boldsymbol{y}}^{(n)},
\end{aligned}
$$
其中 $\hat{\boldsymbol{y}}^{(n)}=\operatorname{softmax}\left(\boldsymbol{W}^{\top} \boldsymbol{x}^{(n)}\right)$ 为样本 $\boldsymbol{x}^{(n)}$ 在每个类别的后验概率.

参数更新采用梯度下降的方式进行更新（P76）：
$$
\boldsymbol{W}_{t+1} \leftarrow \boldsymbol{W}_{t}+\alpha\left(\frac{1}{N} \sum_{n=1}^{N} \boldsymbol{x}^{(n)}\left(\boldsymbol{y}^{(n)}-\hat{\boldsymbol{y}}_{W_{t}}^{(n)}\right)^{\top}\right)
$$
其中 $\alpha$ 是学习率, $\hat{y}_{W_{t}}^{(n)}$ 是当参数为 $\boldsymbol{W}_{t}$ 时, Softmax 回归模型的输出。

如下是训练代码，batch处用numpy的矩阵乘法可以有效加快速度，实测中大约快了5倍。

```python
# 矩阵乘法加速
def train_faster(train_X,train_y,val_X,val_y,batchsize=32,lr=1e0,epoch_number=100):
    iter_number=train_X.shape[0]//batchsize
    iter_remain=train_X.shape[0]%batchsize
    weight=np.zeros((train_X.shape[1],class_number))
    # 不同初始值的影响
    # weight=np.random.normal(0,1,[train_X.shape[1],class_number])
    train_loss_list=[]
    test_loss_list=[]
    for i in range(epoch_number):
        train_loss=0
        test_loss=0
        for j in tqdm(range(iter_number)):
            train_data=train_X[j*batchsize:j*batchsize+batchsize]
            y_train=train_y[j*batchsize:j*batchsize+batchsize]
            y=np.exp(train_data.dot(weight))   
            y_hat=np.divide(y.T,np.sum(y,axis=1)).T
            train_loss+= (-1/train_X.shape[0])*np.sum(np.multiply(y_train,np.log10(y_hat)))
            # 每个batch权重更新一次
            weight+=(lr/batchsize)*train_data.T.dot(y_train-y_hat)
       
        y=np.exp(val_X.dot(weight))   
        y_hat=np.divide(y.T,np.sum(y,axis=1)).T
        test_loss= (-1/val_X.shape[0])*np.sum(np.multiply(val_y,np.log10(y_hat)))
        # print('train_loss:',train_loss," test_loss:",test_loss)
        train_loss_list.append(train_loss)
        test_loss_list.append(test_loss)
    
    return train_loss_list,test_loss_list,weight
```



## 实验

做了7组实验，简单地进行了一下对照，由于机器算力问题，实验都只采用了1000条训练集。采用单词作为特征的模型整体好于用字符。batchsize为32效果较好。同时由于没有做到很好的特征选择，比如用PCA或者是CV的办法进行验证，基本所有的模型都有明显的过拟合现象。实验参数与结果如下。

```python
{'ngram': 2, 'analyzer': 'word', 'batchsize': 32, 'lr': 1.0, 'epoch_number': 20, 'test_rate': 0.2, 'acc': '0.4949', 'best_train_loss': '0.3815', 'best_test_loss': '0.5423', 'path': 'pic/ngram_2-analyzer_word-lr_1.0-batchsize_32-epoch_number_20-test_rate_0.2.jpg'}
```

![](https://gitee.com/AICollector/picgo/raw/master/ngram_2-analyzer_word-lr_1.0-batchsize_32-epoch_number_20-test_rate_0.2.jpg)

```python
{'ngram': 2, 'analyzer': 'char', 'batchsize': 32, 'lr': 1.0, 'epoch_number': 20, 'test_rate': 0.2, 'acc': '0.4949', 'best_train_loss': '0.4033', 'best_test_loss': '0.5376', 'path': 'pic/ngram_2-analyzer_char-lr_1.0-batchsize_32-epoch_number_20-test_rate_0.2.jpg'}
```

![](https://gitee.com/AICollector/picgo/raw/master/ngram_2-analyzer_char-lr_1.0-batchsize_32-epoch_number_20-test_rate_0.2.jpg)

```python
{'ngram': 3, 'analyzer': 'char', 'batchsize': 32, 'lr': 1.0, 'epoch_number': 20, 'test_rate': 0.2, 'acc': '0.5303', 'best_train_loss': '0.3559', 'best_test_loss': '0.5807', 'path': 'pic/ngram_3-analyzer_char-lr_1.0-batchsize_32-epoch_number_20-test_rate_0.2.jpg'}
```

![](https://gitee.com/AICollector/picgo/raw/master/ngram_3-analyzer_char-lr_1.0-batchsize_32-epoch_number_20-test_rate_0.2.jpg)

```python
{'ngram': 2, 'analyzer': 'word', 'batchsize': 8, 'lr': 1.0, 'epoch_number': 20, 'test_rate': 0.2, 'acc': '0.5707', 'best_train_loss': '0.3200', 'best_test_loss': '0.5200', 'path': 'pic/ngram_2-analyzer_word-lr_1.0-batchsize_8-epoch_number_20-test_rate_0.2.jpg'}
```

![](https://gitee.com/AICollector/picgo/raw/master/ngram_2-analyzer_word-lr_1.0-batchsize_8-epoch_number_20-test_rate_0.2.jpg)

```python
{'ngram': 2, 'analyzer': 'word', 'batchsize': 128, 'lr': 1.0, 'epoch_number': 20, 'test_rate': 0.2, 'acc': '0.5000', 'best_train_loss': '0.4699', 'best_test_loss': '0.5582', 'path': 'pic/ngram_2-analyzer_word-lr_1.0-batchsize_128-epoch_number_20-test_rate_0.2.jpg'}
```

![](https://gitee.com/AICollector/picgo/raw/master/ngram_2-analyzer_word-lr_1.0-batchsize_128-epoch_number_20-test_rate_0.2.jpg)

```python
{'ngram': 2, 'analyzer': 'word', 'batchsize': 32, 'lr': 10.0, 'epoch_number': 20, 'test_rate': 0.2, 'acc': '0.4949', 'best_train_loss': '0.2863', 'best_test_loss': '0.6994', 'path': 'pic/ngram_2-analyzer_word-lr_10.0-batchsize_32-epoch_number_20-test_rate_0.2.jpg'}
```

![](https://gitee.com/AICollector/picgo/raw/master/ngram_2-analyzer_word-lr_10.0-batchsize_32-epoch_number_20-test_rate_0.2.jpg)

```python
{'ngram': 2, 'analyzer': 'word', 'batchsize': 32, 'lr': 1.0, 'epoch_number': 10, 'test_rate': 0.2, 'acc': '0.5707', 'best_train_loss': '0.4530', 'best_test_loss': '0.5318', 'path': 'pic/ngram_2-analyzer_word-lr_1.0-batchsize_32-epoch_number_10-test_rate_0.2.jpg'}
```

![](https://gitee.com/AICollector/picgo/raw/master/ngram_2-analyzer_word-lr_1.0-batchsize_32-epoch_number_10-test_rate_0.2.jpg)























