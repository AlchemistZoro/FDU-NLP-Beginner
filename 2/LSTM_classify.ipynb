{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码参考：\n",
    "https://discourse.qingxzd.com/t/pytorch/58\n",
    "\n",
    "辅助参考:\n",
    "https://www.pythonf.cn/read/128035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torchtext.legacy import data,datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import Vectors\n",
    "from torch.nn import init\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import jieba    # 用来预处理文本（分词等）\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=2019\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   # 选择Gpu或Cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/train.tsv', sep='\\t')\n",
    "df_test=pd.read_csv('data/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(df, test_size=0.2)\n",
    "train.to_csv(\"./data/train.csv\", index=False)\n",
    "val.to_csv(\"./data/val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):    \n",
    "    return [wd for wd in jieba.cut(text, cut_all=False)]\n",
    "\n",
    "en_stopwords=stopwords.words('english')\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, stop_words=en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = data.TabularDataset.splits(\n",
    "    path='./data', train='train.csv', validation='val.csv', format='csv', skip_header=True,\n",
    "    fields=[('PhraseId', None), ('SentenceId', None), ('Phrase', TEXT), ('Sentiment', LABEL)]\n",
    ")\n",
    "\n",
    "test = data.TabularDataset('./data/test.tsv', format='tsv', skip_header=True, \n",
    "                           fields=[('PhraseId', None), ('SentenceId', None), ('Phrase', TEXT)])\n",
    "print(len(train),train[2].Phrase, train[2].Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 建立vocab（不需要加载预训练的词向量） \n",
    "# TEXT.build_vocab(train, val)\n",
    "# LABEL.build_vocab(train, val)\n",
    "\n",
    "# 建立vocab（加载预训练的词向量，如果路径没有该词向量，会自动下载）\n",
    "TEXT.build_vocab(train, vectors='glove.6B.100d')#, max_size=30000)\n",
    "# 当 corpus 中有的 token 在 vectors 中不存在时 的初始化方式.\n",
    "TEXT.vocab.vectors.unk_init = init.xavier_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造迭代器\n",
    "'''\n",
    "sort_key指在一个batch内根据文本长度进行排序。\n",
    "'''\n",
    "train_iter = data.BucketIterator(train, batch_size=128, sort_key=lambda x: len(x.Phrase), \n",
    "                                 shuffle=True,device=DEVICE)\n",
    "\n",
    "val_iter = data.BucketIterator(val, batch_size=128, sort_key=lambda x: len(), \n",
    "                                 shuffle=True,device=DEVICE)\n",
    "\n",
    "# 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序\n",
    "test_iter = data.Iterator(dataset=test, batch_size=128, train=False,\n",
    "                          sort=False, device=DEVICE)\n",
    "\n",
    "# 查看trainiter一个batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[2].Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "a= batch.Phrase\n",
    "label = batch.Sentiment\n",
    "print(a.shape)\n",
    "print(batch.Phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文本中的唯一标记\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#标签中唯一令牌的集合\n",
    "# print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#常用单词\n",
    "# print(TEXT.vocab.freqs.most_common(10))\n",
    "\n",
    "# 词库向量的大小\n",
    "print(TEXT.vocab.vectors.shape)\n",
    "#单词词典\n",
    "# print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LSTM_base(nn.Module):\n",
    "    #定义模型中使用的所有层\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        #构造函数\n",
    "        super().__init__()\n",
    "        #embeddding层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #lstm层\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim , output_dim)\n",
    "    def forward(self, text):\n",
    "        #text = [batch size,sent_length]\n",
    "        embedded = self.embedding(text)        \n",
    "        out,_=self.lstm(embedded)\n",
    "        out=self.fc(out[:,-1,:])\n",
    "        #最终激活函数\n",
    "        out = F.softmax(out,-1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_MAX(nn.Module):\n",
    "    #定义模型中使用的所有层\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        #构造函数\n",
    "        super().__init__()\n",
    "        #embeddding层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #lstm层\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim,\n",
    "                           num_layers=1,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2 , output_dim)\n",
    "    def forward(self, text):\n",
    "        #text = [batch size,sent_length]\n",
    "        h_embedding = self.embedding(text)     \n",
    "\n",
    "        h_lstm1, _ = self.lstm(h_embedding)\n",
    "\n",
    "        h_lstm2, _ = self.lstm(h_lstm1)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)\n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        out=self.fc(h_conc)\n",
    "        #最终激活函数\n",
    "        out = F.softmax(out,-1)\n",
    "        return out\n",
    "\n",
    "#定义超参数\n",
    "size_of_vocab = len(TEXT.vocab)\n",
    "embedding_dim = 100\n",
    "num_hidden_nodes = 100\n",
    "num_output_nodes = 5\n",
    "num_layers = 2\n",
    "bidirection = False\n",
    "dropout = 0.4\n",
    "\n",
    "#实例化模型\n",
    "model = LSTM_base(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \n",
    "                   bidirectional = bidirection, dropout = dropout)\n",
    "# model = LSTM_MAX(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \n",
    "#                    bidirectional = bidirection, dropout = dropout)   \n",
    "\n",
    "#模型框架\n",
    "print(model)\n",
    "#可训练参数的数量\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "#初始化预训练的词嵌入\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "print(pretrained_embeddings.shape)  \n",
    "\n",
    "#定义优化器和损失\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#定义度量指标\n",
    "def binary_accuracy(preds, y):\n",
    "\n",
    "    #round预测到最接近的整数\n",
    "    # rounded_preds = torch.round(preds)\n",
    "    correct = (preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "#转化为cuda（如果可用）\n",
    "model = model.to(DEVICE)\n",
    "criterion = criterion.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(model, iterator, optimizer, criterion):\n",
    "    #每个epoch进行初始化\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    #将模型设置为训练阶段\n",
    "    model.train()\n",
    "    predictions_val=[]\n",
    "    for batch in tqdm(iterator):\n",
    "        #重设梯度\n",
    "        optimizer.zero_grad()\n",
    "        #获取文本和单词数量\n",
    "        text = batch.Phrase\n",
    "        text = text.permute(1,0)\n",
    "        # print(text.shape)\n",
    "        #转换为一维张量\n",
    "        predictions = model(text).squeeze()\n",
    "        #计算loss\n",
    "        loss = criterion(predictions, batch.Sentiment)\n",
    "        #计算二分类准确度\n",
    "        predictions_val=  predictions.argmax(dim=1)\n",
    "\n",
    "\n",
    "        acc = binary_accuracy(predictions_val, batch.Sentiment.float())\n",
    "        #后向传播损失并计算梯度\n",
    "        loss.backward()\n",
    "        #更新权重\n",
    "        optimizer.step()\n",
    "        #损失和准确度\n",
    "        \n",
    "        epoch_loss += loss.item()  \n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def Evaluate(model, iterator, criterion):\n",
    "    #每个epoch进行初始化\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    #停用dropout层\n",
    "    model.eval()\n",
    "    #停用自动求导\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator):\n",
    "            #获取文本和单词数量\n",
    "            text = batch.Phrase\n",
    "            text = text.permute(1,0)\n",
    "            #转换为一维张量\n",
    "            predictions = model(text).squeeze()\n",
    "            #计算损失和准确度\n",
    "            loss = criterion(predictions, batch.Sentiment)\n",
    "            predictions_val=   predictions.argmax(dim=1)\n",
    "\n",
    "            acc = binary_accuracy(predictions_val, batch.Sentiment.float())\n",
    "            #跟踪损失和准确度\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def Test(model, iterator, criterion):\n",
    "    #每个epoch进行初始化\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    #停用dropout层\n",
    "    model.eval()\n",
    "    predict_list=[]\n",
    "    #停用自动求导\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator):\n",
    "            #获取文本和单词数量\n",
    "            text = batch.Phrase\n",
    "            text = text.permute(1,0)\n",
    "            #转换为一维张量\n",
    "            predictions = model(text).squeeze()\n",
    "            #计算损失和准确度\n",
    "\n",
    "            predict=   predictions.argmax(dim=1).cpu()\n",
    "            predict_list+=predict.numpy().flatten().tolist()\n",
    "\n",
    "    return predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_loss_list=[]\n",
    "valid_loss_list=[]\n",
    "train_acc_list=[]\n",
    "valid_acc_list=[]\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    #训练模型\n",
    "    print('epoch:',epoch)\n",
    "    train_loss, train_acc = Train(model, train_iter, optimizer, criterion)\n",
    "    #评估模型\n",
    "    valid_loss, valid_acc = Evaluate(model, val_iter, criterion)\n",
    "    #保存模型\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    valid_acc_list.append(valid_acc)\n",
    "\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def drawpic(train_loss_list=[],test_loss_list=[],epoch_number=10,title='1',root_path='./'):\n",
    "    # make data\n",
    "    x = [i for i in range(epoch_number)]\n",
    "    # plot\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(title)\n",
    "    ax.plot(x, train_loss_list, linewidth=2.0)\n",
    "    ax.plot(x, test_loss_list, linewidth=2.0)\n",
    "    path=root_path+title+'.jpg'\n",
    "    print('![]({})'.format(path))\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "root_path='./pic/'\n",
    "title='lstm-glove-loss-{}'.format(str(N_EPOCHS))\n",
    "\n",
    "drawpic(train_loss_list=train_loss_list,test_loss_list=valid_loss_list,epoch_number=N_EPOCHS,title=title,root_path=root_path)\n",
    "\n",
    "title='lstm-glove-acc-{}'.format(str(N_EPOCHS))\n",
    "\n",
    "drawpic(train_loss_list=train_acc_list,test_loss_list=valid_acc_list,epoch_number=N_EPOCHS,title=title,root_path=root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=Test(model, test_iter, criterion)\n",
    "len(predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test=pd.read_csv('./data/sampleSubmission.csv')\n",
    "df_test['Sentiment']=predict\n",
    "df_test.to_csv('./submission.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载权重\n",
    "path='./saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "#推断 \n",
    "\n",
    "def predict(model, text):\n",
    "    tokenized = [wd for wd in jieba.cut(text, cut_all=False)]\n",
    "  #令牌化(tokenize)句子 \n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]          #转换为整数序列\n",
    "    length = [len(indexed)]                                    #计算单词个数\n",
    "    tensor = torch.LongTensor(indexed).to(DEVICE)              #转换为张量\n",
    "    tensor = tensor.unsqueeze(1).T                             #reshape成[batch, 单词个数]\n",
    "    length_tensor = torch.LongTensor(length)                   #转换为张量\n",
    "    prediction = model(tensor)                  #预测\n",
    "    return prediction.item()\n",
    "\n",
    "#进行预测\n",
    "print(predict(model, \"fuck fuck shit bitch?\"))\n",
    "#不真诚的问题\n",
    "print(predict(model, \"Why Indian girls go crazy about marrying Shri. Rahul Gandhi ji?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "a = t.tensor([[1,2,3],[6,0,0],[4,5,0]]) #(batch_size, max_length)\n",
    "lengths = t.tensor([3,1,2])\n",
    "\n",
    "# 排序\n",
    "a_lengths, idx = lengths.sort(0, descending=True)\n",
    "_, un_idx = t.sort(idx, dim=0)\n",
    "a = a[un_idx]\n",
    "\n",
    "# 定义层 \n",
    "emb = t.nn.Embedding(20,2,padding_idx=0) \n",
    "lstm = t.nn.LSTM(input_size=2, hidden_size=4, batch_first=True) \n",
    "fc = nn.Linear(4, 1)\n",
    "        #激活函数\n",
    "act = nn.Sigmoid() \n",
    "\n",
    "a_input = emb(a)\n",
    "a_packed_input = t.nn.utils.rnn.pack_padded_sequence(input=a_input, lengths=a_lengths, batch_first=True)\n",
    "packed_out, _ = lstm(a_packed_input)\n",
    "out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "# 根据un_idx将输出转回原输入顺序\n",
    "out = t.index_select(out, 0, un_idx)\n",
    "linear=fc(out)\n",
    "final=act(linear)\n",
    "print('a_input.shape:',a_input.shape)\n",
    "print('a_packed_input.data.shape',a_packed_input.data.shape)\n",
    "print('packed_out.data.shape:',packed_out.data.shape)\n",
    "print('out.shape:',out.shape)\n",
    "print(_[0].shape,_[1].shape)\n",
    "print(linear.shape)\n",
    "\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
