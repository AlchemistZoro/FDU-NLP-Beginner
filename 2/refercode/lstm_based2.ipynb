{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码参考：\n",
    "https://www.pythonf.cn/read/128035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.legacy import data,datasets\n",
    "import pandas as pd\n",
    "\n",
    "from torchtext.vocab import Vectors\n",
    "from torch.nn import init\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jieba    # 用来预处理文本（分词等）\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=2019\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   # 选择Gpu或Cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/train.tsv', sep='\\t')\n",
    "df_test=pd.read_csv('data/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(df, test_size=0.2)\n",
    "train.to_csv(\"./data/train.csv\", index=False)\n",
    "val.to_csv(\"./data/val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):    \n",
    "    return [wd for wd in jieba.cut(text, cut_all=False)]\n",
    "\n",
    "en_stopwords=stopwords.words('english')\n",
    "\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, stop_words=en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zoro\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.594 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124848 [' ', 'could', ' ', ' ', 'wrapped', ' ', 'things', ' ', ' ', ' ', '80', ' ', 'minutes'] 1\n"
     ]
    }
   ],
   "source": [
    "train, val = data.TabularDataset.splits(\n",
    "    path='./data', train='train.csv', validation='val.csv', format='csv', skip_header=True,\n",
    "    fields=[('PhraseId', None), ('SentenceId', None), ('Phrase', TEXT), ('Sentiment', LABEL)]\n",
    ")\n",
    "\n",
    "test = data.TabularDataset('./data/test.tsv', format='tsv', skip_header=True, \n",
    "                           fields=[('PhraseId', None), ('SentenceId', None), ('Phrase', TEXT)])\n",
    "print(len(train),train[2].Phrase, train[2].Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 建立vocab（不需要加载预训练的词向量） \n",
    "# TEXT.build_vocab(train, val)\n",
    "# LABEL.build_vocab(train, val)\n",
    "\n",
    "# 建立vocab（加载预训练的词向量，如果路径没有该词向量，会自动下载）\n",
    "TEXT.build_vocab(train, vectors='glove.6B.100d')#, max_size=30000)\n",
    "# 当 corpus 中有的 token 在 vectors 中不存在时 的初始化方式.\n",
    "TEXT.vocab.vectors.unk_init = init.xavier_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造迭代器\n",
    "'''\n",
    "sort_key指在一个batch内根据文本长度进行排序。\n",
    "'''\n",
    "train_iter = data.BucketIterator(train, batch_size=128, sort_key=lambda x: len(x.Phrase), \n",
    "                                 shuffle=True,device=DEVICE)\n",
    "\n",
    "val_iter = data.BucketIterator(val, batch_size=128, sort_key=lambda x: len(x.Phrase), \n",
    "                                 shuffle=True,device=DEVICE)\n",
    "\n",
    "# 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序\n",
    "test_iter = data.Iterator(dataset=test, batch_size=128, train=False,\n",
    "                          sort=False, device=DEVICE)\n",
    "\n",
    "# 查看trainiter一个batch\n",
    "\n",
    "# batch = next(iter(train_iter))\n",
    "# a= batch.Phrase\n",
    "# label = batch.Sentiment\n",
    "# print(a.shape)\n",
    "# print(batch.Phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 15160\n",
      "torch.Size([15160, 100])\n"
     ]
    }
   ],
   "source": [
    "#文本中的唯一标记\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "# #标签中唯一令牌的集合\n",
    "# print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#常用单词\n",
    "# print(TEXT.vocab.freqs.most_common(10))\n",
    "\n",
    "# 词库向量的大小\n",
    "print(TEXT.vocab.vectors.shape)\n",
    "# #单词词典\n",
    "# print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 \t batch_idx : 199 \t loss: 2.1322 \t train acc: 0.4688\n",
      "epoch: 0 \t batch_idx : 399 \t loss: 1.9495 \t train acc: 0.5156\n",
      "epoch: 0 \t batch_idx : 599 \t loss: 1.9382 \t train acc: 0.5391\n",
      "epoch: 0 \t batch_idx : 799 \t loss: 2.0284 \t train acc: 0.5156\n",
      "val acc : 0.5144 > 0.0000 saving model\n",
      "val acc: 0.5144\n",
      "epoch: 1 \t batch_idx : 199 \t loss: 2.0110 \t train acc: 0.4922\n",
      "epoch: 1 \t batch_idx : 399 \t loss: 1.9866 \t train acc: 0.4922\n",
      "epoch: 1 \t batch_idx : 599 \t loss: 2.0713 \t train acc: 0.5000\n",
      "epoch: 1 \t batch_idx : 799 \t loss: 2.0416 \t train acc: 0.4922\n",
      "val acc : 0.5144 > 0.5144 saving model\n",
      "val acc: 0.5144\n",
      "epoch: 2 \t batch_idx : 199 \t loss: 2.0745 \t train acc: 0.4297\n",
      "epoch: 2 \t batch_idx : 399 \t loss: 1.8750 \t train acc: 0.5312\n",
      "epoch: 2 \t batch_idx : 599 \t loss: 1.8860 \t train acc: 0.5312\n",
      "epoch: 2 \t batch_idx : 799 \t loss: 1.9514 \t train acc: 0.5234\n",
      "val acc: 0.5144\n",
      "epoch: 3 \t batch_idx : 199 \t loss: 2.0162 \t train acc: 0.5312\n",
      "epoch: 3 \t batch_idx : 399 \t loss: 2.0408 \t train acc: 0.5312\n",
      "epoch: 3 \t batch_idx : 599 \t loss: 2.1525 \t train acc: 0.4688\n",
      "epoch: 3 \t batch_idx : 799 \t loss: 1.9487 \t train acc: 0.5859\n",
      "val acc : 0.5145 > 0.5144 saving model\n",
      "val acc: 0.5145\n",
      "epoch: 4 \t batch_idx : 199 \t loss: 2.0416 \t train acc: 0.4922\n",
      "epoch: 4 \t batch_idx : 399 \t loss: 1.9778 \t train acc: 0.5312\n",
      "epoch: 4 \t batch_idx : 599 \t loss: 1.8840 \t train acc: 0.5469\n",
      "epoch: 4 \t batch_idx : 799 \t loss: 2.0166 \t train acc: 0.4922\n",
      "val acc : 0.5463 > 0.5145 saving model\n",
      "val acc: 0.5463\n",
      "epoch: 5 \t batch_idx : 199 \t loss: 1.9347 \t train acc: 0.5781\n",
      "epoch: 5 \t batch_idx : 399 \t loss: 1.9894 \t train acc: 0.5547\n",
      "epoch: 5 \t batch_idx : 599 \t loss: 1.7958 \t train acc: 0.5703\n",
      "epoch: 5 \t batch_idx : 799 \t loss: 1.8026 \t train acc: 0.5469\n",
      "val acc : 0.5981 > 0.5463 saving model\n",
      "val acc: 0.5981\n",
      "epoch: 6 \t batch_idx : 199 \t loss: 1.3319 \t train acc: 0.6641\n",
      "epoch: 6 \t batch_idx : 399 \t loss: 1.7114 \t train acc: 0.5391\n",
      "epoch: 6 \t batch_idx : 599 \t loss: 1.4575 \t train acc: 0.6797\n",
      "epoch: 6 \t batch_idx : 799 \t loss: 1.5459 \t train acc: 0.6562\n",
      "val acc : 0.6352 > 0.5981 saving model\n",
      "val acc: 0.6352\n",
      "epoch: 7 \t batch_idx : 199 \t loss: 1.4227 \t train acc: 0.6641\n",
      "epoch: 7 \t batch_idx : 399 \t loss: 1.1980 \t train acc: 0.7344\n",
      "epoch: 7 \t batch_idx : 599 \t loss: 1.2576 \t train acc: 0.6953\n",
      "epoch: 7 \t batch_idx : 799 \t loss: 1.4092 \t train acc: 0.7188\n",
      "val acc : 0.6508 > 0.6352 saving model\n",
      "val acc: 0.6508\n",
      "epoch: 8 \t batch_idx : 199 \t loss: 1.2017 \t train acc: 0.7422\n",
      "epoch: 8 \t batch_idx : 399 \t loss: 1.3788 \t train acc: 0.6562\n",
      "epoch: 8 \t batch_idx : 599 \t loss: 1.5716 \t train acc: 0.6484\n",
      "epoch: 8 \t batch_idx : 799 \t loss: 1.2116 \t train acc: 0.7500\n",
      "val acc : 0.6598 > 0.6508 saving model\n",
      "val acc: 0.6598\n",
      "epoch: 9 \t batch_idx : 199 \t loss: 1.2910 \t train acc: 0.6953\n",
      "epoch: 9 \t batch_idx : 399 \t loss: 1.5121 \t train acc: 0.6250\n",
      "epoch: 9 \t batch_idx : 599 \t loss: 1.0948 \t train acc: 0.7500\n",
      "epoch: 9 \t batch_idx : 799 \t loss: 1.2813 \t train acc: 0.6875\n",
      "val acc : 0.6642 > 0.6598 saving model\n",
      "val acc: 0.6642\n",
      "epoch: 10 \t batch_idx : 199 \t loss: 1.2721 \t train acc: 0.6875\n",
      "epoch: 10 \t batch_idx : 399 \t loss: 1.1831 \t train acc: 0.7578\n",
      "epoch: 10 \t batch_idx : 599 \t loss: 1.3415 \t train acc: 0.6953\n",
      "epoch: 10 \t batch_idx : 799 \t loss: 1.1404 \t train acc: 0.7422\n",
      "val acc: 0.6579\n",
      "epoch: 11 \t batch_idx : 199 \t loss: 1.2855 \t train acc: 0.6953\n",
      "epoch: 11 \t batch_idx : 399 \t loss: 1.2523 \t train acc: 0.7188\n",
      "epoch: 11 \t batch_idx : 599 \t loss: 1.2368 \t train acc: 0.7266\n",
      "epoch: 11 \t batch_idx : 799 \t loss: 1.2931 \t train acc: 0.7031\n",
      "val acc: 0.6615\n",
      "epoch: 12 \t batch_idx : 199 \t loss: 1.2524 \t train acc: 0.7734\n",
      "epoch: 12 \t batch_idx : 399 \t loss: 1.1882 \t train acc: 0.7188\n",
      "epoch: 12 \t batch_idx : 599 \t loss: 1.2064 \t train acc: 0.7188\n",
      "epoch: 12 \t batch_idx : 799 \t loss: 1.1668 \t train acc: 0.7109\n",
      "val acc: 0.6617\n",
      "epoch: 13 \t batch_idx : 199 \t loss: 0.9255 \t train acc: 0.7734\n",
      "epoch: 13 \t batch_idx : 399 \t loss: 1.0806 \t train acc: 0.7734\n",
      "epoch: 13 \t batch_idx : 599 \t loss: 1.2769 \t train acc: 0.7344\n",
      "epoch: 13 \t batch_idx : 799 \t loss: 1.0591 \t train acc: 0.7734\n",
      "val acc: 0.6511\n",
      "epoch: 14 \t batch_idx : 199 \t loss: 1.0841 \t train acc: 0.7656\n",
      "epoch: 14 \t batch_idx : 399 \t loss: 0.9884 \t train acc: 0.7656\n",
      "epoch: 14 \t batch_idx : 599 \t loss: 1.1857 \t train acc: 0.7031\n",
      "epoch: 14 \t batch_idx : 799 \t loss: 1.1352 \t train acc: 0.7422\n",
      "val acc: 0.6578\n",
      "epoch: 15 \t batch_idx : 199 \t loss: 0.9304 \t train acc: 0.7812\n",
      "epoch: 15 \t batch_idx : 399 \t loss: 1.2429 \t train acc: 0.7500\n",
      "epoch: 15 \t batch_idx : 599 \t loss: 1.1496 \t train acc: 0.7500\n",
      "epoch: 15 \t batch_idx : 799 \t loss: 1.0668 \t train acc: 0.7578\n",
      "val acc: 0.6534\n",
      "epoch: 16 \t batch_idx : 199 \t loss: 0.8541 \t train acc: 0.8281\n",
      "epoch: 16 \t batch_idx : 399 \t loss: 1.0241 \t train acc: 0.8047\n",
      "epoch: 16 \t batch_idx : 599 \t loss: 1.2141 \t train acc: 0.7266\n",
      "epoch: 16 \t batch_idx : 799 \t loss: 1.1099 \t train acc: 0.7422\n",
      "val acc: 0.6535\n",
      "epoch: 17 \t batch_idx : 199 \t loss: 0.7578 \t train acc: 0.8438\n",
      "epoch: 17 \t batch_idx : 399 \t loss: 0.7800 \t train acc: 0.7969\n",
      "epoch: 17 \t batch_idx : 599 \t loss: 0.9229 \t train acc: 0.7969\n",
      "epoch: 17 \t batch_idx : 799 \t loss: 1.0694 \t train acc: 0.7734\n",
      "val acc: 0.6451\n",
      "epoch: 18 \t batch_idx : 199 \t loss: 0.8366 \t train acc: 0.8438\n",
      "epoch: 18 \t batch_idx : 399 \t loss: 0.7823 \t train acc: 0.8281\n",
      "epoch: 18 \t batch_idx : 599 \t loss: 0.9056 \t train acc: 0.7969\n",
      "epoch: 18 \t batch_idx : 799 \t loss: 0.8568 \t train acc: 0.8359\n",
      "val acc: 0.6490\n",
      "epoch: 19 \t batch_idx : 199 \t loss: 0.8193 \t train acc: 0.8047\n",
      "epoch: 19 \t batch_idx : 399 \t loss: 0.7763 \t train acc: 0.8750\n",
      "epoch: 19 \t batch_idx : 599 \t loss: 0.9946 \t train acc: 0.7656\n",
      "epoch: 19 \t batch_idx : 799 \t loss: 0.8936 \t train acc: 0.7969\n",
      "val acc: 0.6467\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "由于目的是学习torchtext的使用，所以只定义了一个简单模型\n",
    "\"\"\"\n",
    "len_vocab = len(TEXT.vocab)\n",
    "\n",
    "class Enet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Enet, self).__init__()\n",
    "        self.embedding = nn.Embedding(len_vocab,100)\n",
    "        self.lstm = nn.LSTM(100,128,3,batch_first=True)#,bidirectional=True)\n",
    "        self.linear = nn.Linear(128,5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size,seq_num = x.shape\n",
    "        vec = self.embedding(x)\n",
    "        out, (hn, cn) = self.lstm(vec)\n",
    "        out = self.linear(out[:,-1,:])\n",
    "        out = F.softmax(out,-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Enet()\n",
    "\"\"\"\n",
    "将前面生成的词向量矩阵拷贝到模型的embedding层\n",
    "这样就自动的可以将输入的word index转为词向量\n",
    "如果没有使用预训练词向量，name就用随机生成的，会跟着模型进行更新\n",
    "vocab_size是所用词的总数，embedding_dim是预设的词向量维度。\n",
    "model.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\"\"\"\n",
    "model.embedding.weight.data.copy_(TEXT.vocab.vectors)   \n",
    "model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 训练\n",
    "optimizer = optim.Adam(model.parameters())#,lr=0.000001)\n",
    "\n",
    "n_epoch = 20\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        data = batch.Phrase\n",
    "        target = batch.Sentiment\n",
    "        target = torch.sparse.torch.eye(5).index_select(dim=0, index=target.cpu().data)\n",
    "        target = target.to(DEVICE)\n",
    "        data = data.permute(1,0)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(data)\n",
    "        loss = -target*torch.log(out)-(1-target)*torch.log(1-out)\n",
    "        loss = loss.sum(-1).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx+1) %200 == 0:\n",
    "            _,y_pre = torch.max(out,-1)\n",
    "            acc = torch.mean((torch.tensor(y_pre == batch.Sentiment,dtype=torch.float)))\n",
    "            print('epoch: %d \\t batch_idx : %d \\t loss: %.4f \\t train acc: %.4f'\n",
    "                  %(epoch,batch_idx,loss,acc))\n",
    "    \n",
    "    val_accs = []\n",
    "    for batch_idx, batch in enumerate(val_iter):\n",
    "        data = batch.Phrase\n",
    "        target = batch.Sentiment\n",
    "        target = torch.sparse.torch.eye(5).index_select(dim=0, index=target.cpu().data)\n",
    "        target = target.to(DEVICE)\n",
    "        data = data.permute(1,0)\n",
    "        out = model(data)\n",
    "        \n",
    "        _,y_pre = torch.max(out,-1)\n",
    "        acc = torch.mean((torch.tensor(y_pre == batch.Sentiment,dtype=torch.float))).cpu()\n",
    "        val_accs.append(acc)\n",
    "    \n",
    "    acc = np.array(val_accs).mean()\n",
    "    if acc > best_val_acc:\n",
    "        print('val acc : %.4f > %.4f saving model'%(acc,best_val_acc))\n",
    "        torch.save(model.state_dict(), 'params.pkl')\n",
    "        best_val_acc = acc\n",
    "    print('val acc: %.4f'%(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
