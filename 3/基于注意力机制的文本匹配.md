# 任务三：基于注意力机制的文本匹配

输入两个句子判断，判断它们之间的关系。参考[ESIM]( https://arxiv.org/pdf/1609.06038v3.pdf)（可以只用LSTM，忽略Tree-LSTM），用双向的注意力机制实现。

1. 参考
   1. 《[神经网络与深度学习](https://nndl.github.io/)》 第7章
   2. Reasoning about Entailment with Neural Attention <https://arxiv.org/pdf/1509.06664v1.pdf>
   3. Enhanced LSTM for Natural Language Inference <https://arxiv.org/pdf/1609.06038v3.pdf>
2. 数据集：https://nlp.stanford.edu/projects/snli/
3. 实现要求：Pytorch
4. 知识点：
   1. 注意力机制
   2. token2token attetnion
5. 时间：两周

---

学习流程：

第一步：

1、什么是注意力机制、有哪些实现方式。

2、 怎么用其做推断

第二步：

完成句子推断的代码

第三步：

根据论文的公式对代码进行修正

---

https://github.com/bentrevett/pytorch-nli

网络上的开源代码用的是Bilstm网络。用网络上的开源代码进行测试，为了让电脑能够运行代码，更改了部分参数：

1. glove改成100维
2. train的个数改为10000个
3. 然后最后几层的线性层不要

逻辑就是将假设与推断之间分别通过一个lstm网络然后把最后的输出合并在一起。

所以现在的问题就是如何使用注意力机制：？

---



论文阅读：

Enhanced LSTM for Natural Language Inference

符号定义：

In our notation, we have two sentences $\mathbf{a}=$ $\left(\mathbf{a}_{1}, \ldots, \mathbf{a}_{\ell_{a}}\right)$ and $\mathbf{b}=\left(\mathbf{b}_{1}, \ldots, \mathbf{b}_{\ell_{b}}\right)$, where $\mathbf{a}$ is a premise and $\mathbf{b}$ a hypothesis. The $\mathbf{a}_{i}$ or $\mathbf{b}_{j} \in \mathbb{R}^{l}$ is an embedding of $l$-dimensional vector, which can be initialized with some pre-trained word embeddings and organized with parse trees. The goal is to predict a label $y$ that indicates the logic relationship between $\mathbf{a}$ and $\mathbf{b}$.

框架：

![image-20220121000524160](https://gitee.com/AICollector/picgo/raw/master/image-20220121000524160.png)

步骤：

1. Input Encoding
2. Local Inference Modeling
3. Inference Composition

**Input Encoding**

encode the input premise and hypothesis。输出的是最后的一个隐状态，如果是双向lstm的话应该是最后两个隐状态。
$$
\begin{aligned}
\overline{\mathbf{a}}_{i} &=\operatorname{BiLSTM}(\mathbf{a}, i), \forall i \in\left[1, \ldots, \ell_{a}\right] \\
\overline{\mathbf{b}}_{j} &=\operatorname{BiLSTM}(\mathbf{b}, j), \forall j \in\left[1, \ldots, \ell_{b}\right]
\end{aligned}
$$

这里的使用的是lstm的每个隐状态而不是最后一层的隐状态。


**Local Inference Modeling/attention**

1. Locality of inference

parikn那篇文章使用预训练号的词库然后进行注意力机制的使用然后直接训练。缺点是没法捕获上下文的信息。

利用注意力机制对输入进行解码，从而获得更好的效果。注意力矩阵e就是对两个隐状态直接进行相乘。在parikn那篇文章中需要在套上一层的MLP。
$$
e_{i j}=\overline{\mathbf{a}}_{i}^{T} \overline{\mathbf{b}}_{j}
$$
e的作用：obtain the local relevance between a premise and hypothesis.



2. Local inference collected over sequences 



对于前提中的每个单词的隐状态，用注意力矩阵e和bj相乘来获得和假设的相关性。

即：relevant semantics in the hypothesis
$$
\begin{aligned}
\tilde{\mathbf{a}}_{i} &=\sum_{j=1}^{\ell_{b}} \frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{\ell_{b}} \exp \left(e_{i k}\right)} \overline{\mathbf{b}}_{j}, \forall i \in\left[1, \ldots, \ell_{a}\right] \\
\tilde{\mathbf{b}}_{j} &=\sum_{i=1}^{\ell_{a}} \frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{\ell_{a}} \exp \left(e_{k j}\right)} \overline{\mathbf{a}}_{i}, \forall j \in\left[1, \ldots, \ell_{b}\right]
\end{aligned}
$$
第一个公式是前提在假设中的语义，所以是乘以假设的向量。



3. Enhancement of local inference information

希望能够对模型有所提升，原话是：

>We expect that such operations could help sharpen local inference information between elements in the tuples and capture inference relationships such as contradiction
>
>This process could be regarded as a special case of modeling some high-order interaction between the tuple elements


$$
\begin{aligned}
\mathbf{m}_{a} &=[\overline{\mathbf{a}} ; \tilde{\mathbf{a}} ; \overline{\mathbf{a}}-\tilde{\mathbf{a}} ; \overline{\mathbf{a}} \odot \tilde{\mathbf{a}}] \\
\mathbf{m}_{b} &=[\overline{\mathbf{b}} ; \tilde{\mathbf{b}} ; \overline{\mathbf{b}}-\tilde{\mathbf{b}} ; \overline{\mathbf{b}} \odot \tilde{\mathbf{b}}]
\end{aligned}
$$
**Inference Composition**

1. composition layer

同样是接一层BiLSTM， 和Encoding一样，但是输入的是上面的ma，mb。
$$
\begin{aligned}
\mathbf{v}_{a,i} &=\operatorname{BiLSTM}(\mathbf{m_a}, i), \forall i \in\left[1, \ldots, \ell_{a}\right] \\
\mathbf{v}_{b,j} &=\operatorname{BiLSTM}(\mathbf{m_b}, j), \forall j \in\left[1, \ldots, \ell_{b}\right]
\end{aligned}
$$


2. pooling/aggregating

$$
\begin{aligned}
\mathbf{v}_{a, \mathrm{ave}} &=\sum_{i=1}^{\ell_{a}} \frac{\mathbf{v}_{a, i}}{\ell_{a}}, \quad \mathbf{v}_{a, \max }=\max _{i=1}^{\ell_{a}} \mathbf{v}_{a, i} \\
\mathbf{v}_{b, \mathrm{ave}} &=\sum_{j=1}^{\ell_{b}} \frac{\mathbf{v}_{b, j}}{\ell_{b}}, \quad \mathbf{v}_{b, \max }=\max _{j=1}^{\ell_{b}} \mathbf{v}_{b, j} \\
\mathbf{v} &=\left[\mathbf{v}_{a, \mathrm{ave}} ; \mathbf{v}_{a, \max } ; \mathbf{v}_{b, \mathrm{ave}} ; \mathbf{v}_{b, \max }\right]
\end{aligned}
$$

3. MLP

v再通过一个Mlp，MLP由如下的内容组成：

1. 线性网络

2. 激活函数：tanh

3. 输出层：softmax 。也就是分到各类的一个概率。

#### Experiment

loss:multi-class cross-entropy loss.

optimization：adam。第一轮0.9 第二轮 0.999。不太明白是什么意思。

learning rate ： 0.0004 

batch size ： 32

embeddings ：All hidden states of LSTMs, tree-LSTMs, and word embeddings have 300 dimensions

dropout ： 0.5, which is applied to all feedforward connections. 

We use pre-trained 300-D Glove 840B vectorsto initialize our word embeddings. Out-of-vocabulary (OOV) words are initialized randomly with Gaussian samples. 

All vectors including word embedding are updated during training.

实验的结果：

![image-20220121022538846](https://gitee.com/AICollector/picgo/raw/master/image-20220121022538846.png)

---

实践：

基本的代码按照给的模板就行，但是也存在问题即参数的大小对不上。官方的参数量是4.3M，而我这边的参数量是15M。

可能原因：

1. 在dropout部分设计有误。

2. 查看每一层的参数，vocab的量似乎出了问题。实际中使用的个数应当没有25000+个单词，应该是更小一些，总之也没时间看了，先去做下一题了。



---

训练结果：

测试集上85%的水平离论文中给出的结果88%有不小的差距。

![](https://gitee.com/AICollector/picgo/raw/master/ESMI-loss-5.jpg)

![](https://gitee.com/AICollector/picgo/raw/master/ESMI-acc-5.jpg)

----
已有代码：
Attention-LSTM.ipynb 从网上拷贝的代码实现，没有使用attention机制
D2L-AddAttention.ipynb D2L 加性注意力机制的实现，和方案没有什么关系，用于理解注意力机制的
ESIM.ipynb 代码实现，使用文中给的注意力机制