{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy import data,datasets\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED=2022\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences(path,change_dict):\n",
    "    \n",
    "    sentences=[]\n",
    "    sentence=''\n",
    "    keys=list(change_dict.keys())\n",
    "    values=list(change_dict.values())\n",
    "    for line in codecs.open(path, 'r', 'utf-8'):     \n",
    "        if line=='\\n':\n",
    "            sentences.append(sentence)\n",
    "            sentence=''\n",
    "        else:\n",
    "            line=line[0:-1]\n",
    "            line=re.sub(keys[0],values[0],line)\n",
    "            line=re.sub(keys[1],values[1],line)\n",
    "            sentence+=line\n",
    "    return sentences[1:]\n",
    "\n",
    "def load_dataset(sentences,valid_rate=0.2,fix_length=10):\n",
    "    phrases=[]\n",
    "    next_phrases=[]\n",
    "    for sentence in sentences:\n",
    "        sen_len=len(sentence)\n",
    "        for i in range(sen_len-fix_length-1):\n",
    "            phrases.append(sentence[i:i+fix_length])\n",
    "            next_phrases.append(sentence[i+1:i+fix_length+1])\n",
    "    dataset=pd.DataFrame()\n",
    "    dataset['text']=phrases\n",
    "    dataset['label']=next_phrases\n",
    "    train_data=dataset.sample(frac=(1-valid_rate),random_state=0,axis=0)\n",
    "    valid_data=dataset[~dataset.index.isin(train_data.index)]\n",
    "    return train_data,valid_data,dataset\n",
    "def laod_dataset(df,textfield,labelfield):\n",
    "\n",
    "    examples=[]\n",
    "    fields = [('text', textfield), ('label', labelfield)]\n",
    "    for index,row in df.iterrows():\n",
    "        examples.append(data.Example.fromlist([row['text'], row['label']], fields))\n",
    "    \n",
    "    dataset = data.Dataset(examples, fields)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIX_LENGTH=10\n",
    "BATCH_SIZE=32\n",
    "EMBEDDING_DIM=300\n",
    "VALID_RATE=0.2\n",
    "\n",
    "\n",
    "#因为预训练模型里面有没，。需要转化\n",
    "change_dict={\n",
    "    '。':'○',\n",
    "    '，':','\n",
    "}\n",
    "CORPUS_PATH='../data/poetryFromTang.txt'\n",
    "sentences=load_sentences(path=CORPUS_PATH,change_dict=change_dict)\n",
    "\n",
    "def tokenizer(text):\n",
    "    return list(text)\n",
    "\n",
    "TEXT=data.Field(sequential=True,tokenize=tokenizer,use_vocab=True)\n",
    "# LABEL=data.Field(sequential=True,tokenize=tokenizer,use_vocab=True)\n",
    "\n",
    "train_df,valid_df,df=load_dataset(sentences,valid_rate=VALID_RATE,fix_length=FIX_LENGTH)\n",
    "train_data=laod_dataset(train_df,textfield=TEXT,labelfield=TEXT)\n",
    "valid_data=laod_dataset(valid_df,textfield=TEXT,labelfield=TEXT)\n",
    "dataset=laod_dataset(df,textfield=TEXT,labelfield=TEXT)\n",
    "\n",
    "\n",
    "if not os.path.exists('.vector_cache'):\n",
    "    os.mkdir('.vector_cache') \n",
    "vectors = Vectors(name='../data/sgns.sikuquanshu.word') \n",
    "\n",
    "\n",
    "#dataset/vector/unk_init\n",
    "TEXT.build_vocab(dataset,vectors=vectors)\n",
    "# LABEL.build_vocab(dataset,vectors=vectors)\n",
    "\n",
    "train_iterator=data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=DEVICE,\n",
    "    sort_key=lambda x: len(x.TEXT)\n",
    ")\n",
    "\n",
    "valid_iterator=data.BucketIterator(\n",
    "    valid_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=DEVICE,\n",
    "    sort_key=lambda x: len(x.TEXT)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_data[1].text)\n",
    "print(valid_data[1].label)\n",
    "print(' ')\n",
    "\n",
    "batch=next(iter(valid_iterator))\n",
    "text=batch.text\n",
    "label=batch.label\n",
    "\n",
    "for  i  in range(BATCH_SIZE):\n",
    "    print([TEXT.vocab.itos[t] for t in text[:,i]])\n",
    "    print([TEXT.vocab.itos[t] for t in label[:,i]])\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_BASE(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.lstm=nn.LSTM(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          bidirectional=False,\n",
    "                          batch_first=True,\n",
    "                          )\n",
    "        self.fc=nn.Linear(hidden_dim,vocab_size)\n",
    "    def forward(self,text):\n",
    "        embedding_text=self.embedding(text)\n",
    "        out,_=self.lstm(embedding_text)\n",
    "        out=self.fc(out)\n",
    "        return out\n",
    "\n",
    "class GRU_BASE(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.gru=nn.GRU(embedding_dim,hidden_dim,batch_first=True)\n",
    "        self.fc=nn.Linear(hidden_dim,vocab_size)\n",
    "    def forward(self,text):\n",
    "        embedding_text=self.embedding(text)\n",
    "        out,_=self.gru(embedding_text)\n",
    "        out=self.fc(out)\n",
    "        return out\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "HIDDEN_DIM = 300\n",
    "\n",
    "MODEL_NAME='GRU'\n",
    "\n",
    "# model=LSTM_BASE(\n",
    "#     vocab_size=INPUT_DIM,\n",
    "#     embedding_dim=EMBEDDING_DIM,\n",
    "#     hidden_dim=HIDDEN_DIM,\n",
    "\n",
    "#      ).to(DEVICE)\n",
    "\n",
    "model=GRU_BASE(\n",
    "    vocab_size=INPUT_DIM,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "     ).to(DEVICE)     \n",
    "\n",
    "\n",
    "# Embedding set\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# dont no why use this ,set some vector to zero\n",
    "# model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.requires_grad = True\n",
    "\n",
    "\n",
    "# Optimizer and LossFunction\n",
    "# use the paper best parameter\n",
    "optimizer = optim.Adam(model.parameters(),lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# Evaluation function \n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    correct=torch.tensor(correct.sum()).to(DEVICE)\n",
    "    all=torch.tensor(y.shape[0]).to(DEVICE)\n",
    "    return correct/ all\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0  \n",
    "    model.train() \n",
    "    for batch in tqdm(iterator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        text = batch.text\n",
    "        text=text.permute(1,0)\n",
    "        label = batch.label\n",
    "        label = label.permute(1,0)\n",
    "        predictions = model(text)\n",
    "\n",
    "        predictions=predictions.view(FIX_LENGTH*label.shape[0],-1)\n",
    "        label = label.reshape(FIX_LENGTH*label.shape[0])\n",
    "\n",
    "        loss = criterion(predictions, label)\n",
    "                        \n",
    "        acc = categorical_accuracy(predictions, label)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in tqdm(iterator):\n",
    "\n",
    "            text = batch.text\n",
    "            text=text.permute(1,0)\n",
    "            label = batch.label\n",
    "            label = label.permute(1,0)\n",
    "            predictions = model(text)\n",
    "\n",
    "            predictions=predictions.view(FIX_LENGTH*label.shape[0],-1)\n",
    "            label = label.reshape(FIX_LENGTH*label.shape[0])\n",
    "\n",
    "            loss = criterion(predictions, label)\n",
    "                            \n",
    "            acc = categorical_accuracy(predictions, label)\n",
    "          \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainmode():\n",
    "\n",
    "    N_EPOCHS = 100\n",
    "\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "\n",
    "    train_loss_list=[]\n",
    "    valid_loss_list=[]\n",
    "    train_acc_list=[]\n",
    "    valid_acc_list=[]\n",
    "    train_pp_list=[]\n",
    "    valid_pp_list=[]\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        train_pp=np.exp(train_loss)\n",
    "        valid_pp=np.exp(valid_loss)\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), '../model/{}-model.pt'.format(MODEL_NAME))\n",
    "        \n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "        train_pp_list.append(train_pp)\n",
    "        valid_pp_list.append(valid_pp)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}| Train PP: {train_pp:.2f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f}| Val PP: {valid_pp:.2f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "\n",
    "    def drawpic(train_loss_list=[],test_loss_list=[],epoch_number=10,title='1',root_path='./'):\n",
    "        # make data\n",
    "        x = [i for i in range(epoch_number)]\n",
    "        # plot\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.title(title)\n",
    "        ax.plot(x, train_loss_list, linewidth=2.0)\n",
    "        ax.plot(x, test_loss_list, linewidth=2.0)\n",
    "        path=root_path+title+'.jpg'\n",
    "        print('![]({})'.format(path))\n",
    "        plt.savefig(path)\n",
    "        plt.show()\n",
    "    root_path='../pic/'\n",
    "    title='{}-loss-{}'.format(MODEL_NAME,str(N_EPOCHS))\n",
    "\n",
    "    drawpic(train_loss_list=train_loss_list,test_loss_list=valid_loss_list,epoch_number=N_EPOCHS,title=title,root_path=root_path)\n",
    "\n",
    "    title='{}-acc-{}'.format(MODEL_NAME,str(N_EPOCHS))\n",
    "\n",
    "    drawpic(train_loss_list=train_acc_list,test_loss_list=valid_acc_list,epoch_number=N_EPOCHS,title=title,root_path=root_path)\n",
    "\n",
    "    title='{}-PP-{}'.format(MODEL_NAME,str(N_EPOCHS))\n",
    "\n",
    "    drawpic(train_loss_list=train_acc_list,test_loss_list=valid_acc_list,epoch_number=N_EPOCHS,title=title,root_path=root_path)\n",
    "\n",
    "trainmode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru=GRU_BASE(\n",
    "    vocab_size=INPUT_DIM,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "     ).to(DEVICE)   \n",
    "\n",
    "\n",
    "model_gru.load_state_dict(torch.load('../model/GRU-model.pt'))\n",
    "\n",
    "batch=next(iter(valid_iterator))\n",
    "text = batch.text\n",
    "text=text.permute(1,0)\n",
    "label = batch.label\n",
    "label = label.permute(1,0)\n",
    "predictions = model_gru(text)\n",
    "\n",
    "\n",
    "# predictions=predictions.view(FIX_LENGTH*label.shape[0],-1)\n",
    "max_preds = predictions.argmax(dim = 2, keepdim = True)\n",
    "\n",
    "print(max_preds.shape)\n",
    "for i in range(32):\n",
    "    sen=text[i]\n",
    "    sen_pre=max_preds[i].reshape(FIX_LENGTH)\n",
    "    lab=label[i]\n",
    "    sen_pre1=[TEXT.vocab.itos[int(t)] for t in sen_pre]\n",
    "    sen1=[TEXT.vocab.itos[int(t)] for t in sen]\n",
    "    lab1=[TEXT.vocab.itos[int(t)] for t in lab]\n",
    "    print(''.join(sen1),'  ',''.join(lab1),' ',''.join(sen_pre1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='勇敢牛牛，不怕困难'\n",
    "sentence=re.sub('，',',',sentence)\n",
    "sentence=re.sub('。','○',sentence)\n",
    "sen_list=list(sentence)\n",
    "sen_len=len(sen_list)\n",
    "if sen_len>FIX_LENGTH:\n",
    "    sen_list=sen_list[sen_len-FIX_LENGTH:sen_len]\n",
    "elif sen_len<FIX_LENGTH:\n",
    "    sen_list=['○' for i in range(FIX_LENGTH-sen_len)]+sen_list\n",
    "indexed = [TEXT.vocab.stoi[t] for t in sen_list] \n",
    "\n",
    "tensor = torch.LongTensor(indexed).to(DEVICE)              #转换为张量\n",
    "tensor = tensor.unsqueeze(1).T                             #reshape成[batch, 单词个数]\n",
    "poem_idx=[]\n",
    "poem_len=1000\n",
    "for i in range(poem_len):\n",
    "    prediction = model_gru(tensor)\n",
    "    max_preds = prediction.argmax(dim = 2, keepdim = True)\n",
    "    max_preds=max_preds.reshape(FIX_LENGTH)\n",
    "    poem_idx.append(int(max_preds[-1].cpu()))\n",
    "    tensor=torch.cat([tensor[:,1:FIX_LENGTH], torch.LongTensor([[max_preds[-1]]]).cuda()], 1)\n",
    "\n",
    "\n",
    "\n",
    "sen_predict=[TEXT.vocab.itos[t] for t in poem_idx]\n",
    "sen_predict=''.join(sen_predict)\n",
    "sen_predict=re.sub(',','，',sen_predict)\n",
    "sen_predict=re.sub('○','。',sen_predict)\n",
    "\n",
    "sentence=sentence+sen_predict\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b69a98d3df882577ba469635c4ab08c5ae67eaedfd3a57f311f98966a6edb2d0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
