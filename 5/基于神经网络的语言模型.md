### 任务五：基于神经网络的语言模型

用LSTM、GRU来训练字符级的语言模型，计算困惑度

1. 参考
   1. 《[神经网络与深度学习](https://nndl.github.io/)》 第6、15章
2. 数据集：poetryFromTang.txt
3. 实现要求：Pytorch
4. 知识点：
   1. 语言模型：困惑度等
   2. 文本生成
5. 时间：两周



---
数据处理：

torchtext中文模型加载：https://www.cnblogs.com/cxq1126/p/14392401.html

torchtext教程：https://blog.csdn.net/leo_95/article/details/87708267

```python
if not os.path.exists(.vector_cache):
    os.mkdir(.vector_cache)
vectors = Vectors(name='myvector/glove/glove.6B.200d.txt')
TEXT.build_vocab(train, vectors=vectors)
```



关于困惑度以及生成模型：

慢学NLP / 语言模型RNN-LM (Pytorch-batch) - 乐天的文章 - 知乎 https://zhuanlan.zhihu.com/p/59821641

这个词‘剺’在诗歌文本里面显示不出来，手动替换了



当语料比较少的时候，如果用train_data进行vocab_build的时候，validdata的label就是0。但是为什么官方给出的例子里面都是用train进行vocab_build的？

---

实验结果：用lstm的话，最后出现loss上涨的现象。而且效果没有GRU好。

optimizer = optim.Adam(model.parameters(),lr=5e-5)

optimizer = optim.Adam(model.parameters(),lr=4e-4)

用前者达到的指标比后者要好很多，但是当lr再变小的时候就没有用了。正确率和困惑度都没法进一步增加了。

GRU-model.pt 是lr=5e-5的时候训练出来的。

问题解决：
```python
optimizer.zero_grad()
```
忘了加这一句了，所以之后开始直接无法收敛


---

在这个任务中，不需要有两个Field，就一个TEXT就可以了，因为训练和测试用的是一套的词库，所以不需要。





